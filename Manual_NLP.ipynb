{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNzmr5Up/koDj9ryr5mU90J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/idv713/jupyter-exploration/blob/main/Manual_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "YajC4ecXPb1D"
      },
      "outputs": [],
      "source": [
        "# Manual NLP Pipeline from Corpus to Vectorization\n",
        "# (This is a placeholder for your actual code. Replace with your specific implementation)\n",
        "\n",
        "# 1. Corpus Creation (e.g., from text files)\n",
        "corpus = [\"This is a sample sentence.\", \"Another sentence for the corpus.\"]\n",
        "\n",
        "# 2. Preprocessing\n",
        "# (e.g., tokenization, stemming, removing stop words)\n",
        "\n",
        "# 3. Feature Extraction\n",
        "# (e.g., TF-IDF, word embeddings)\n",
        "\n",
        "# 4. Vectorization\n",
        "# (e.g., converting text into numerical vectors)\n",
        "\n",
        "# ... (Your specific NLP pipeline code)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U pip setuptools wheel\n",
        "!pip install -U spacy\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dQwVSmMTdXb",
        "outputId": "6fbcac9f-f9a7-4f13-eab1-2b6b6adff5df"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (25.0.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (78.1.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (0.45.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.4)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (78.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
            "Requirement already satisfied: blis<1.3.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "k0LXmGy2Udg3"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "obUhcJLtPgYY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"This is an example sentence.\"\n",
        "doc = nlp(text)\n",
        "for token in doc:\n",
        "      print(token.text, token.pos_, token.dep_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hIwt7jRPVClR",
        "outputId": "07429f97-5670-44bf-dd13-18bc17c8a91f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This PRON nsubj\n",
            "is AUX ROOT\n",
            "an DET det\n",
            "example NOUN compound\n",
            "sentence NOUN attr\n",
            ". PUNCT punct\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a file named 'my_corpus.txt' and write some text into it\n",
        "with open('my_corpus.txt', 'w', encoding='utf-8') as file:\n",
        "   file.write(\"This is the content of my corpus file.\")\n",
        "\n",
        "with open('my_corpus.txt', 'r', encoding='utf-8') as file:\n",
        "   corpus = file.read()\n",
        "   print(corpus)\n",
        "\n",
        "with open('my_corpus.txt', 'r', encoding='utf-8') as file:\n",
        "   corpus = file.read()\n",
        "   print(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2nzXfGkdkZL",
        "outputId": "8ce1644d-4fbf-4949-fb62-53dba6169702"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is the content of my corpus file.\n",
            "This is the content of my corpus file.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "with open('my_corpus.txt', 'r', encoding='utf-8') as file:\n",
        "     corpus = file.read()\n",
        "\n",
        "     doc = nlp(corpus)\n",
        "\n",
        "     for token in doc:\n",
        "         print(token.text, token.pos_, token.dep_)\n",
        "\n",
        "     doc = nlp(corpus)\n",
        "\n",
        "     for token in doc:\n",
        "        print(token.text, token.pos_, token.dep_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vheijOyQghXl",
        "outputId": "44ad80fa-0f8f-492b-ec59-ec322ad01d5c"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This PRON nsubj\n",
            "is AUX ROOT\n",
            "the DET det\n",
            "content NOUN attr\n",
            "of ADP prep\n",
            "my PRON poss\n",
            "corpus NOUN compound\n",
            "file NOUN pobj\n",
            ". PUNCT punct\n",
            "This PRON nsubj\n",
            "is AUX ROOT\n",
            "the DET det\n",
            "content NOUN attr\n",
            "of ADP prep\n",
            "my PRON poss\n",
            "corpus NOUN compound\n",
            "file NOUN pobj\n",
            ". PUNCT punct\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "text = \"This is an example sentence with running and jumped words.\"\n",
        "doc = nlp(text)\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "text = \"This is an example sentence with running and jumped words.\"\n",
        "doc = nlp(text)\n",
        "\n",
        "#Lemmatization and Stop Word Removal\n",
        "processed_text = [token.lemma_ for token in doc if not token.is_stop]\n",
        "\n",
        "print(processed_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgYVKjcRk9_R",
        "outputId": "0daf1152-5ddc-4298-dd71-f80bf480f163"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['example', 'sentence', 'run', 'jump', 'word', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "text = \"This is an example sentence\"\n",
        "doc = nlp(text)\n",
        "\n",
        "#Get word embeddings for each token\n",
        "for token in doc:\n",
        "    print(token.text, token.vector)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O3T2WleTnHeb",
        "outputId": "b9b025d9-da53-4893-9940-282c6a26c4d5"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This [-0.32775763 -0.53253955  0.3985917   0.75695336  0.25980037 -0.09992863\n",
            "  0.20512463 -0.4758266  -0.6246184  -0.52172655  0.49154168  1.3910044\n",
            "  0.7877766  -0.3876686  -1.0412033  -0.61645603 -0.8892325   0.7112517\n",
            " -0.6901633   0.8641378  -0.86415005  0.7099123  -0.640488   -0.05409375\n",
            " -0.799928   -0.22031868  0.5761447  -0.0083012  -0.32881686  1.2609969\n",
            "  0.00938568 -0.82335764 -0.09914917 -0.41896957 -0.1634347   0.1749025\n",
            " -1.0782188   0.26848686  0.29795465  2.1400864  -0.17261243 -0.8024565\n",
            " -1.161236    1.2514268  -0.10505679  0.4688578   0.01834872  2.006647\n",
            "  0.79294    -0.57917756 -0.3252145   0.09428576 -0.043593    0.6025034\n",
            "  0.81915206 -0.6480706  -0.03941834 -0.474921   -0.35007912  0.12766045\n",
            "  0.32538587 -0.5288007  -0.09595476 -1.3014529   0.15760022 -0.6431366\n",
            "  1.4132321  -1.2239789  -0.61311233 -1.0903547  -0.93378425  0.55998844\n",
            " -0.46104205  0.02772035 -0.18140063 -0.26479435 -0.07560301  0.01317132\n",
            "  1.0720471  -0.76792073  0.37419403 -1.4470252   0.47849527  0.05491047\n",
            "  1.0060799   1.3401823   0.53433526 -0.3391894   1.6586267   0.05250733\n",
            " -0.20975427 -1.1909373   0.81159925 -0.23905456 -0.44600296  0.23698972]\n",
            "is [ 0.99078643  0.77351266 -0.1115506  -0.28487855  1.0925827  -0.75516176\n",
            " -0.02157599  0.22970632 -0.12786078 -0.3017923   0.26610264 -0.6436658\n",
            " -0.55465025  0.7277422   0.03795497  0.58915883 -0.5122163  -0.36779508\n",
            " -0.75499296  0.58963466 -0.573614   -0.7763317  -0.8480181   0.4523987\n",
            " -0.8306761   0.08624164 -0.30279842  0.42635193  0.3656673   0.17191519\n",
            " -0.57666016  1.522949    0.31379908 -0.4700109   0.04242378 -0.2859842\n",
            " -0.1588423  -0.7613648   0.517296   -0.45388868  1.8802348   0.23286575\n",
            "  1.3365915  -0.45107928  1.2738371  -0.06793669  1.6427178  -0.04829203\n",
            "  0.15684932 -0.78278404 -0.765135    1.4135733  -0.78825223 -0.06249245\n",
            " -0.7387387  -0.6381528  -0.3258443  -0.8220804   0.06254286 -0.38454738\n",
            "  0.9521403   0.2433988   0.13066533  1.1235348   0.24659818  0.5988055\n",
            " -0.46253088 -0.55552953  1.086577   -0.11965105 -0.5333383   0.6829904\n",
            " -0.17808776  0.85283643 -0.3717759   0.15769845 -0.02288407 -0.9802989\n",
            " -0.01970414 -0.5568416  -0.28437474 -0.06463894 -1.2372478   0.65829945\n",
            " -0.03617344  0.14422289 -0.28470346  0.12528983 -0.41348946 -0.5287297\n",
            " -0.00741604 -0.8976852  -0.4215234  -0.13770305 -0.5151881   0.6189215 ]\n",
            "an [ 1.4176135   0.36925098  1.3384086   0.7252069  -0.4294092   0.22986215\n",
            " -0.26172453 -0.20449299  0.23747058  0.37289816 -0.2564793   1.2445753\n",
            "  1.3929253   1.9004232  -0.01607407 -0.63288426 -0.7990116   1.3164296\n",
            "  0.40156955 -0.09183695  0.06993121 -0.5319288  -0.00964721 -0.55594796\n",
            "  0.18139136 -0.42696697  0.82696307 -0.9819945   2.4619727   0.56962746\n",
            " -0.46582544  0.78298867 -0.35696465 -0.7510816   0.08004676 -0.04937696\n",
            "  0.38595855  0.6571876  -0.59438485  0.05677474 -0.28431517  0.74868953\n",
            " -0.11750923 -0.9791523  -0.41008383  0.8009408  -0.50273037  0.18917431\n",
            " -0.3002785  -0.2936483  -0.87018704 -0.23775817  0.15011105  1.1202701\n",
            "  0.54890525  0.37138593 -1.0240319   0.13080391 -0.5886228  -0.06252366\n",
            " -0.64933455 -0.8926629   0.18826962 -1.009439   -0.4237054  -0.6799249\n",
            " -0.01488627 -0.62810975  0.2848555  -0.4371348   0.18173285 -0.07821252\n",
            " -0.15042453 -0.28826457 -1.0889888   0.04064471 -0.24519075  0.4402398\n",
            " -0.18007594 -0.16189729 -0.95811534  0.0337743  -0.16114151 -0.7284944\n",
            " -0.16387025  0.16855626  0.46465975 -0.85628587  0.9001862   0.77991843\n",
            " -0.5780325  -0.53468347 -0.33740333 -0.887884   -1.0016364  -0.09449519]\n",
            "example [ 4.82517481e-01 -1.24670112e+00  1.99310422e+00  3.67295414e-01\n",
            "  7.88806677e-02 -8.61703232e-02  8.01564574e-01  2.46314287e-01\n",
            " -2.38276586e-01  1.40807927e-01 -1.30319595e+00 -1.00333273e-01\n",
            "  2.09417015e-01  2.59214640e-01  3.07947099e-01  5.58539212e-01\n",
            " -4.76904988e-01 -9.60220098e-01 -2.07060575e-01 -8.68305683e-01\n",
            " -4.17938352e-01  3.28562111e-01 -3.36786509e-02 -8.18846107e-01\n",
            " -1.92204565e-01  1.06080234e-01 -3.28242451e-01 -6.05563939e-01\n",
            "  1.99782479e+00  5.36441326e-01 -7.88326740e-01 -2.02694029e-01\n",
            " -2.76916295e-01 -8.87748152e-02 -8.17841887e-02  3.31576049e-01\n",
            " -3.37199748e-01 -5.93776643e-01 -2.98086941e-01 -3.17299962e-02\n",
            "  2.59030089e-02  6.57824636e-01  5.20394087e-01  7.48956919e-01\n",
            "  2.22563922e-01 -5.29453158e-04 -7.43553042e-03 -1.55380845e-01\n",
            "  3.21517169e-01 -5.96406937e-01 -4.01413560e-01 -3.35396051e-01\n",
            "  1.28120387e+00 -8.07960391e-01  4.14660573e-01  5.44416189e-01\n",
            "  3.60120595e-01 -2.52175421e-01 -9.16795880e-02 -9.14604425e-01\n",
            " -1.92461908e-03 -7.85889328e-01 -1.66957736e-01 -1.42961133e+00\n",
            "  2.52313197e-01 -3.62059176e-01  4.53592539e-01  4.87408280e-01\n",
            "  1.01928636e-02 -8.09894681e-01  1.52763140e+00 -1.71213105e-01\n",
            " -4.06231061e-02 -1.89417571e-01  1.14519626e-01 -4.52255756e-01\n",
            " -2.75637895e-01 -8.89484107e-01  9.37170863e-01  3.77417356e-02\n",
            " -9.32214499e-01 -7.99378097e-01  8.74931455e-01 -2.38308504e-01\n",
            "  3.24839801e-02  1.82474852e-01  8.76937628e-01 -3.08495104e-01\n",
            " -2.64502615e-01  3.26479703e-01 -2.45619968e-01  1.14369042e-01\n",
            "  1.13120389e+00  2.13341892e-01 -6.45056725e-01  4.93379831e-01]\n",
            "sentence [ 9.6623600e-04 -8.1515181e-01  6.5634236e-02 -4.3026462e-01\n",
            "  5.9185725e-01  3.5989365e-01  1.3604063e+00  3.8749695e-01\n",
            " -3.0007386e-01 -2.2311199e-01 -4.8095509e-01 -1.8550234e-01\n",
            " -8.3620697e-01  5.2163833e-01  2.4727213e-01  5.7743859e-01\n",
            " -6.7656869e-01 -5.2386856e-01 -8.1232734e-02  7.6207960e-01\n",
            "  6.8709004e-01  1.2061080e+00 -4.9840808e-03 -3.6585370e-01\n",
            " -1.0988915e-01 -4.5619023e-01  5.3264558e-02 -2.3885757e-01\n",
            "  5.8776248e-01  9.4005489e-01 -3.2184857e-01  3.8551497e-01\n",
            " -7.8813571e-01  2.1379901e-01 -9.1396451e-02  1.1297132e-01\n",
            "  1.8907291e-01  5.1536240e-02 -2.9067010e-02  6.4660072e-01\n",
            "  4.1543263e-01  9.7847968e-02  1.5258682e-01  4.0713423e-01\n",
            " -5.5680111e-02 -3.7095535e-01 -1.3662195e+00 -6.8369603e-01\n",
            " -7.2845662e-01  2.1122915e-01 -7.3371165e-02 -4.9355704e-01\n",
            "  9.4115108e-01 -4.7600833e-01  8.2360816e-01 -1.0722661e+00\n",
            "  6.0327637e-01  6.3161492e-01 -1.4486611e-02 -5.0965548e-01\n",
            " -1.1741147e+00  6.2679493e-01 -1.0535946e-01  6.2811351e-01\n",
            "  2.8417993e-01 -2.1879807e-01 -8.4329247e-01 -6.1319971e-01\n",
            " -2.2420472e-01  3.3987045e-02  3.9073262e-01  6.4395201e-01\n",
            "  3.7549531e-01 -3.5835776e-01 -4.8266226e-01 -6.5951782e-01\n",
            "  4.8680961e-02 -4.9139065e-01 -1.4371726e-01 -7.8422129e-01\n",
            " -2.9772457e-01 -5.4422036e-02 -6.4844859e-01 -4.3579280e-01\n",
            " -2.4784964e-02  6.9252014e-01  1.3913161e-01 -7.8796166e-01\n",
            "  1.6946420e-02  4.5490593e-01 -6.7322725e-01  6.7577302e-01\n",
            "  1.5602182e+00  2.1825266e-01 -9.0747631e-01  4.3960893e-01]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the spacy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "#Read the corpus from the file\n",
        "with open('my_corpus.txt', 'r', encoding='utf-8') as file:\n",
        "     corpus = file.read()\n",
        "\n",
        "# Process the corpus\n",
        "doc = nlp(corpus)\n",
        "\n",
        "#Lemmatization and Stop Word Removal\n",
        "processed_tokens = [token.lemma_ for token in doc if not token.is_stop]\n",
        "\n",
        "#print the processed tokens\n",
        "print(\"Processed Tokens:\", processed_tokens)\n",
        "\n",
        "#Get word embeddings for the processed tokens\n",
        "for token_text in processed_tokens:\n",
        "    token = nlp.vocab[token_text]\n",
        "\n",
        "#Get the token object from vocabulary\n",
        "if token.has_vector: # Check if the token has a vector\n",
        "   print(token_text, token.vector)\n",
        "else:\n",
        "   print(f\"Token '{token_text}' does not have a vector.\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "faIf4PikoU_4",
        "outputId": "3754f588-788d-496a-a4b5-e1972712a68e"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed Tokens: ['content', 'corpus', 'file', '.']\n",
            "Token '.' does not have a vector.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "#1. Load spaCy model and data\n",
        "nlp = spacy.load('en_core_web_sm\"')\n",
        "\n",
        "# (Replace with your actual data loading and labeling)\n",
        "texts = [\"This is a positive sentence.\", \"This is a negative sentence.\", \"Another positive one.\"]\n",
        "labels = [1, 0, 1] #1 for positive, 0 for negative\n",
        "\n",
        "#2. preprocessing and Feature Extraction\n",
        "def preprocess_text(text):\n",
        "    doc = nlp(text)\n",
        "    return[token.lemma_ for token in doc if not token_is stop]\n",
        "\n",
        "    text_features = [preprocess_text(text) for text in texts)]\n",
        "\n",
        "    #Average word embeddings for document-level features\n",
        "    document_embeddings = []\n",
        "    for tokens in text_features:\n",
        "         embeddings = [nlp.vocab[token].vector for token in tokens if nlp vocb token].has-vector ]\n"
      ],
      "metadata": {
        "id": "7gaLDAe1trZm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}