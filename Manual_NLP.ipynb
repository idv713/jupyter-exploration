{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOLasqcTSC44DFCvLypMwyO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/idv713/jupyter-exploration/blob/main/Manual_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TcbhMiS9nzax",
        "outputId": "59973ed7-3595-4fc2-99aa-1a48dadc5252"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.4)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
            "Requirement already satisfied: blis<1.3.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "#Process a text string\n",
        "text = \"Spacy is a great library for NLP!\"\n",
        "doc = nlp(text)\n",
        "\n",
        "# Tokenize and print the result\n",
        "tokens = [token.text for token in doc]\n",
        "#Iterate through the tokens to print details of each token:\n",
        "for token in doc:\n",
        "    print(f\"text: {token,text}, Lemma: {token.lemma_}, POS: {token.pos_}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qoL6WW-ol5_",
        "outputId": "ebe67894-6fe5-48d9-908d-84ac371718e2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text: (Spacy, 'Spacy is a great library for NLP!'), Lemma: spacy, POS: NOUN\n",
            "text: (is, 'Spacy is a great library for NLP!'), Lemma: be, POS: AUX\n",
            "text: (a, 'Spacy is a great library for NLP!'), Lemma: a, POS: DET\n",
            "text: (great, 'Spacy is a great library for NLP!'), Lemma: great, POS: ADJ\n",
            "text: (library, 'Spacy is a great library for NLP!'), Lemma: library, POS: NOUN\n",
            "text: (for, 'Spacy is a great library for NLP!'), Lemma: for, POS: ADP\n",
            "text: (NLP, 'Spacy is a great library for NLP!'), Lemma: NLP, POS: PROPN\n",
            "text: (!, 'Spacy is a great library for NLP!'), Lemma: !, POS: PUNCT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "id": "nHtkTECEs_Bd",
        "outputId": "7c1b404b-eaf1-4ac1-c6f7-c9c8019deb27"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'corpus'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-9f104b9f6dd0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'corpus'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('corpus') # Downloads the corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHj3xMZVtmHe",
        "outputId": "708ac175-ae11-4fc6-9081-db6154a7b1c7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Error loading corpus: Package 'corpus' not found in index\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "source": [
        "# Task\n",
        "Tell me about this dataset.\n",
        "\n",
        "Here is all the data you need:\n",
        "\"corpus.txt\""
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "7IThORj-ucuN"
      }
    },
    {
      "source": [
        "## Data loading\n",
        "\n",
        "### Subtask:\n",
        "Load the data from the \"corpus.txt\" file into a string variable.\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "9bTqWEd3uc99"
      }
    },
    {
      "source": [
        "**Reasoning**:\n",
        "I need to read the content of \"corpus.txt\" into a string variable named `corpus_text`.\n",
        "\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "5cmeaB8eudN1"
      }
    },
    {
      "source": [
        "try:\n",
        "    with open('corpus.txt', 'r') as file:\n",
        "      corpus_text = file.read()\n",
        "except FileNotFoundError:\n",
        "    corpus_text = None\n",
        "    print(\"Error: corpus.txt not found.\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "id": "j0s1-vRouddn",
        "outputId": "cb1b5fd9-6f94-4a98-e0dc-dc0c468d65ef"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnicodeDecodeError",
          "evalue": "'utf-8' codec can't decode byte 0xa0 in position 469: invalid start byte",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-0cb36ba01f70>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'corpus.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m       \u001b[0mcorpus_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcorpus_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xa0 in position 469: invalid start byte"
          ]
        }
      ]
    },
    {
      "source": [
        "**Reasoning**:\n",
        "The previous attempt failed due to a `UnicodeDecodeError`.  I'll try opening the file with a different encoding, such as 'latin-1' or 'iso-8859-1', which are more likely to handle special characters.\n",
        "\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "jeyUm0jMue6r"
      }
    },
    {
      "source": [
        "try:\n",
        "    with open('corpus.txt', 'r', encoding='latin-1') as file:\n",
        "        corpus_text = file.read()\n",
        "except FileNotFoundError:\n",
        "    corpus_text = None\n",
        "    print(\"Error: corpus.txt not found.\")\n",
        "except UnicodeDecodeError:\n",
        "    try:\n",
        "        with open('corpus.txt', 'r', encoding='iso-8859-1') as file:\n",
        "            corpus_text = file.read()\n",
        "    except UnicodeDecodeError:\n",
        "        corpus_text = None\n",
        "        print(\"Error: Could not decode corpus.txt with latin-1 or iso-8859-1 encoding.\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "LPoG7uGGufKa"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "source": [
        "## Data exploration\n",
        "\n",
        "### Subtask:\n",
        "Explore the loaded text data to understand its basic characteristics.\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWmaPii0uhHZ"
      }
    },
    {
      "source": [
        "**Reasoning**:\n",
        "Explore the loaded text data by calculating the number of characters, words, and sentences, printing the first 200 characters, and analyzing character frequencies.\n",
        "\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "JAx2xSSLuh42"
      }
    },
    {
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "if corpus_text:\n",
        "    num_chars = len(corpus_text)\n",
        "    print(f\"Number of characters: {num_chars}\")\n",
        "\n",
        "    words = re.findall(r'\\b\\w+\\b', corpus_text.lower())\n",
        "    num_words = len(words)\n",
        "    print(f\"Number of words: {num_words}\")\n",
        "\n",
        "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|\\!)\\s', corpus_text)\n",
        "    num_sentences = len(sentences)\n",
        "    print(f\"Number of sentences: {num_sentences}\")\n",
        "\n",
        "    print(f\"First 200 characters:\\n{corpus_text[:200]}\")\n",
        "\n",
        "    char_counts = Counter(corpus_text)\n",
        "    top_10_chars = char_counts.most_common(10)\n",
        "    print(\"Top 10 most frequent characters:\")\n",
        "    for char, count in top_10_chars:\n",
        "        print(f\"'{char}': {count}\")\n",
        "else:\n",
        "    print(\"The text data could not be loaded, so the analysis cannot be performed.\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lA2i-hxuiIk",
        "outputId": "c73e4d79-dd71-4b09-889c-f0d62e13ecd8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of characters: 3889\n",
            "Number of words: 583\n",
            "Number of sentences: 1\n",
            "First 200 characters:\n",
            "march madness predictions ncaa bracket expert picks spread odds thursday sweet \n",
            "best bets duke-arizona florida maryland alabama byu \n",
            "might bemoan lack cinderella second weekend ncaa tournament consola\n",
            "Top 10 most frequent characters:\n",
            "' ': 561\n",
            "'e': 399\n",
            "'a': 326\n",
            "'n': 243\n",
            "'s': 233\n",
            "'t': 226\n",
            "'i': 213\n",
            "'r': 203\n",
            "'o': 181\n",
            "'l': 162\n"
          ]
        }
      ]
    },
    {
      "source": [
        "## Data wrangling\n",
        "\n",
        "### Subtask:\n",
        "Clean the loaded text data.\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "7kV2ZaGvulIC"
      }
    },
    {
      "source": [
        "**Reasoning**:\n",
        "Clean the text data by removing special characters, handling newline characters, converting to lowercase, and removing extra whitespaces.\n",
        "\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "OF1i8iD3ul7X"
      }
    },
    {
      "source": [
        "import re\n",
        "\n",
        "if corpus_text:\n",
        "    # Remove special characters, keeping letters, numbers, and common punctuation\n",
        "    corpus_text = re.sub(r'[^\\w\\s.,!?;:-]', '', corpus_text)\n",
        "\n",
        "    # Replace newline characters with spaces\n",
        "    corpus_text = corpus_text.replace('\\n', ' ').replace('\\r', '')\n",
        "\n",
        "    # Convert to lowercase\n",
        "    corpus_text = corpus_text.lower()\n",
        "\n",
        "    # Remove extra whitespaces\n",
        "    corpus_text = re.sub(r'\\s+', ' ', corpus_text).strip()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "32MYQxrpumLF"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "source": [
        "## Data analysis\n",
        "\n",
        "### Subtask:\n",
        "Analyze the cleaned text data to gain insights.\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "4VjUbyEIuoMc"
      }
    },
    {
      "source": [
        "**Reasoning**:\n",
        "Calculate word frequencies, analyze sentence lengths, identify frequent words excluding stop words, and report other relevant statistics.\n",
        "\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "5w3ePoDJuphw"
      }
    },
    {
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "if corpus_text:\n",
        "    # 1. Word Frequencies\n",
        "    words = re.findall(r'\\b\\w+\\b', corpus_text)\n",
        "    word_counts = Counter(words)\n",
        "    print(\"Word Frequencies:\")\n",
        "    for word, count in word_counts.most_common():\n",
        "        print(f\"{word}: {count}\")\n",
        "\n",
        "    # 2. Sentence Length Distribution\n",
        "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|\\!)\\s', corpus_text)\n",
        "    sentence_lengths = [len(sentence.split()) for sentence in sentences if sentence.strip()]\n",
        "    avg_sentence_length = sum(sentence_lengths) / len(sentence_lengths) if sentence_lengths else 0\n",
        "    min_sentence_length = min(sentence_lengths) if sentence_lengths else 0\n",
        "    max_sentence_length = max(sentence_lengths) if sentence_lengths else 0\n",
        "    print(f\"\\nAverage Sentence Length: {avg_sentence_length}\")\n",
        "    print(f\"Minimum Sentence Length: {min_sentence_length}\")\n",
        "    print(f\"Maximum Sentence Length: {max_sentence_length}\")\n",
        "\n",
        "    # 3. Top 10 Frequent Words (excluding stop words)\n",
        "    stop_words = {\"the\", \"a\", \"an\", \"is\", \"are\", \"of\", \"in\", \"to\", \"and\", \"that\", \"it\", \"for\", \"with\", \"on\", \"as\", \"this\", \"be\", \"at\", \"by\", \"from\", \"or\", \"an\", \"i\", \"you\", \"he\", \"she\", \"we\", \"they\", \"me\", \"him\", \"her\", \"us\", \"them\"}\n",
        "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "    filtered_word_counts = Counter(filtered_words)\n",
        "    top_10_words = filtered_word_counts.most_common(10)\n",
        "    print(\"\\nTop 10 Most Frequent Words (excluding stop words):\")\n",
        "    for word, count in top_10_words:\n",
        "        print(f\"{word}: {count}\")\n",
        "\n",
        "    # 4. Other Observations\n",
        "    print(\"\\nOther Observations:\")\n",
        "    print(\"The text appears to be about NCAA March Madness predictions.\")\n",
        "else:\n",
        "    print(\"The text data could not be loaded, so the analysis cannot be performed.\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGAWVbFKupxd",
        "outputId": "4c41a1e5-af15-469d-c7e2-c334b3a6480f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Frequencies:\n",
            "tournament: 10\n",
            "byu: 8\n",
            "seed: 8\n",
            "march: 7\n",
            "regional: 7\n",
            "madness: 6\n",
            "duke: 6\n",
            "florida: 6\n",
            "game: 6\n",
            "arkansas: 6\n",
            "team: 6\n",
            "arizona: 5\n",
            "maryland: 5\n",
            "back: 5\n",
            "ncaa: 4\n",
            "best: 4\n",
            "alabama: 4\n",
            "will: 4\n",
            "games: 4\n",
            "night: 4\n",
            "coming: 4\n",
            "big: 4\n",
            "left: 4\n",
            "vs: 4\n",
            "live: 4\n",
            "pick: 4\n",
            "thursday: 3\n",
            "sweet: 3\n",
            "playing: 3\n",
            "east: 3\n",
            "west: 3\n",
            "one: 3\n",
            "after: 3\n",
            "first: 3\n",
            "get: 3\n",
            "texas: 3\n",
            "tech: 3\n",
            "season: 3\n",
            "win: 3\n",
            "can: 3\n",
            "going: 3\n",
            "picks: 2\n",
            "spread: 2\n",
            "might: 2\n",
            "cinderella: 2\n",
            "weekend: 2\n",
            "eight: 2\n",
            "programs: 2\n",
            "five: 2\n",
            "have: 2\n",
            "combined: 2\n",
            "span: 2\n",
            "gets: 2\n",
            "started: 2\n",
            "newark: 2\n",
            "true: 2\n",
            "thanks: 2\n",
            "late: 2\n",
            "star: 2\n",
            "freshman: 2\n",
            "derik: 2\n",
            "queen: 2\n",
            "two: 2\n",
            "history: 2\n",
            "blue: 2\n",
            "devils: 2\n",
            "well: 2\n",
            "final: 2\n",
            "coach: 2\n",
            "john: 2\n",
            "opening: 2\n",
            "looks: 2\n",
            "every: 2\n",
            "points: 2\n",
            "bet: 2\n",
            "cbs: 2\n",
            "since: 2\n",
            "feb: 2\n",
            "oddsmakers: 2\n",
            "size: 2\n",
            "efficient: 2\n",
            "read: 2\n",
            "just: 2\n",
            "performance: 2\n",
            "demin: 2\n",
            "matchup: 2\n",
            "tbs: 2\n",
            "giving: 2\n",
            "minutes: 2\n",
            "take: 2\n",
            "hit: 2\n",
            "able: 2\n",
            "plays: 2\n",
            "flagg: 2\n",
            "with: 2\n",
            "shooting: 2\n",
            "predictions: 1\n",
            "bracket: 1\n",
            "expert: 1\n",
            "odds: 1\n",
            "bets: 1\n",
            "bemoan: 1\n",
            "lack: 1\n",
            "second: 1\n",
            "consolation: 1\n",
            "tiny: 1\n",
            "dancer: 1\n",
            "tears: 1\n",
            "schedule: 1\n",
            "loaded: 1\n",
            "heavyweight: 1\n",
            "matchups: 1\n",
            "across: 1\n",
            "during: 1\n",
            "won: 1\n",
            "national: 1\n",
            "championships: 1\n",
            "last: 1\n",
            "years: 1\n",
            "account: 1\n",
            "nearly: 1\n",
            "third: 1\n",
            "titles: 1\n",
            "offensive: 1\n",
            "fireworks: 1\n",
            "new: 1\n",
            "jersey: 1\n",
            "followed: 1\n",
            "shortly: 1\n",
            "taking: 1\n",
            "terps: 1\n",
            "delivered: 1\n",
            "buzzer: 1\n",
            "beater: 1\n",
            "heroics: 1\n",
            "caps: 1\n",
            "square: 1\n",
            "familiar: 1\n",
            "foe: 1\n",
            "carries: 1\n",
            "program: 1\n",
            "personal: 1\n",
            "wildcat: 1\n",
            "former: 1\n",
            "north: 1\n",
            "carolina: 1\n",
            "guard: 1\n",
            "caleb: 1\n",
            "love: 1\n",
            "features: 1\n",
            "double: 1\n",
            "digit: 1\n",
            "entire: 1\n",
            "but: 1\n",
            "hard: 1\n",
            "sec: 1\n",
            "led: 1\n",
            "hall: 1\n",
            "fame: 1\n",
            "calipari: 1\n",
            "razorbacks: 1\n",
            "face: 1\n",
            "most: 1\n",
            "consistent: 1\n",
            "grant: 1\n",
            "mccasland: 1\n",
            "reach: 1\n",
            "elite: 1\n",
            "time: 1\n",
            "head: 1\n",
            "storylines: 1\n",
            "enough: 1\n",
            "college: 1\n",
            "basketball: 1\n",
            "fan: 1\n",
            "portion: 1\n",
            "many: 1\n",
            "make: 1\n",
            "breaking: 1\n",
            "spreads: 1\n",
            "totals: 1\n",
            "selected: 1\n",
            "all: 1\n",
            "four: 1\n",
            "semifinal: 1\n",
            "along: 1\n",
            "selections: 1\n",
            "committee: 1\n",
            "experts: 1\n",
            "ahead: 1\n",
            "number: 1\n",
            "consistently: 1\n",
            "stretch: 1\n",
            "know: 1\n",
            "cougars: 1\n",
            "loss: 1\n",
            "houston: 1\n",
            "performances: 1\n",
            "exceeding: 1\n",
            "expectations: 1\n",
            "too: 1\n",
            "against: 1\n",
            "same: 1\n",
            "small: 1\n",
            "sample: 1\n",
            "view: 1\n",
            "teams: 1\n",
            "country: 1\n",
            "month: 1\n",
            "per: 1\n",
            "bart: 1\n",
            "torvik: 1\n",
            "should: 1\n",
            "decided: 1\n",
            "possessions: 1\n",
            "x: 1\n",
            "factor: 1\n",
            "cover: 1\n",
            "straight: 1\n",
            "foot: 1\n",
            "egor: 1\n",
            "great: 1\n",
            "ball: 1\n",
            "nba: 1\n",
            "draft: 1\n",
            "buzz: 1\n",
            "high: 1\n",
            "level: 1\n",
            "makes: 1\n",
            "nightmare: 1\n",
            "given: 1\n",
            "rounds: 1\n",
            "think: 1\n",
            "carry: 1\n",
            "by: 1\n",
            "chip: 1\n",
            "patterson: 1\n",
            "7: 1\n",
            "hrs: 1\n",
            "ago5: 1\n",
            "min: 1\n",
            "depth: 1\n",
            "mismatch: 1\n",
            "impossible: 1\n",
            "ignore: 1\n",
            "leans: 1\n",
            "starting: 1\n",
            "anyone: 1\n",
            "else: 1\n",
            "deeper: 1\n",
            "rotation: 1\n",
            "bigs: 1\n",
            "cause: 1\n",
            "problems: 1\n",
            "tight: 1\n",
            "whistle: 1\n",
            "low: 1\n",
            "gators: 1\n",
            "endure: 1\n",
            "couple: 1\n",
            "cheap: 1\n",
            "fouls: 1\n",
            "alex: 1\n",
            "condon: 1\n",
            "rueben: 1\n",
            "chinyelu: 1\n",
            "thomas: 1\n",
            "waugh: 1\n",
            "micah: 1\n",
            "handlogten: 1\n",
            "capable: 1\n",
            "good: 1\n",
            "bench: 1\n",
            "if: 1\n",
            "julian: 1\n",
            "reese: 1\n",
            "foul: 1\n",
            "trouble: 1\n",
            "rebounding: 1\n",
            "efforts: 1\n",
            "huge: 1\n",
            "way: 1\n",
            "totally: 1\n",
            "change: 1\n",
            "gained: 1\n",
            "real: 1\n",
            "confidence: 1\n",
            "punch: 1\n",
            "champs: 1\n",
            "uconn: 1\n",
            "money: 1\n",
            "table: 1\n",
            "walter: 1\n",
            "clayton: 1\n",
            "alijah: 1\n",
            "martin: 1\n",
            "richard: 1\n",
            "showed: 1\n",
            "collective: 1\n",
            "experience: 1\n",
            "making: 1\n",
            "winning: 1\n",
            "cooper: 1\n",
            "healthy: 1\n",
            "resulted: 1\n",
            "failed: 1\n",
            "three: 1\n",
            "acc: 1\n",
            "injured: 1\n",
            "early: 1\n",
            "quarterfinals: 1\n",
            "ruled: 1\n",
            "next: 1\n",
            "contests: 1\n",
            "now: 1\n",
            "introduce: 1\n",
            "pushes: 1\n",
            "pace: 1\n",
            "slip: 1\n",
            "little: 1\n",
            "defensive: 1\n",
            "side: 1\n",
            "end: 1\n",
            "regular: 1\n",
            "expecting: 1\n",
            "plenty: 1\n",
            "chasing: 1\n",
            "happens: 1\n",
            "free: 1\n",
            "throw: 1\n",
            "total: 1\n",
            "things: 1\n",
            "numbers: 1\n",
            "explain: 1\n",
            "like: 1\n",
            "how: 1\n",
            "losses: 1\n",
            "string: 1\n",
            "together: 1\n",
            "wins: 1\n",
            "kansas: 1\n",
            "st: 1\n",
            "behind: 1\n",
            "arc: 1\n",
            "ready: 1\n",
            "war: 1\n",
            "believing: 1\n",
            "each: 1\n",
            "other: 1\n",
            "finding: 1\n",
            "ways: 1\n",
            "even: 1\n",
            "shots: 1\n",
            "falling: 1\n",
            "news: 1\n",
            "adou: 1\n",
            "thiero: 1\n",
            "available: 1\n",
            "play: 1\n",
            "limited: 1\n",
            "insignificant: 1\n",
            "need: 1\n",
            "versatile: 1\n",
            "defenders: 1\n",
            "space: 1\n",
            "dangerous: 1\n",
            "offense: 1\n",
            "red: 1\n",
            "raiders: 1\n",
            "give: 1\n",
            "difference: 1\n",
            "jonas: 1\n",
            "aidoo: 1\n",
            "trevon: 1\n",
            "brazile: 1\n",
            "z: 1\n",
            "let: 1\n",
            "jt: 1\n",
            "toppin: 1\n",
            "clear: 1\n",
            "inside: 1\n",
            "chance: 1\n",
            "extend: 1\n",
            "run: 1\n",
            "\n",
            "Average Sentence Length: 581.0\n",
            "Minimum Sentence Length: 581\n",
            "Maximum Sentence Length: 581\n",
            "\n",
            "Top 10 Most Frequent Words (excluding stop words):\n",
            "tournament: 10\n",
            "byu: 8\n",
            "seed: 8\n",
            "march: 7\n",
            "regional: 7\n",
            "madness: 6\n",
            "duke: 6\n",
            "florida: 6\n",
            "game: 6\n",
            "arkansas: 6\n",
            "\n",
            "Other Observations:\n",
            "The text appears to be about NCAA March Madness predictions.\n"
          ]
        }
      ]
    },
    {
      "source": [
        "## Summary:\n",
        "\n",
        "### 1. Q&A\n",
        "\n",
        "The provided data analysis task doesn't explicitly pose questions. However, the analysis implicitly seeks to understand the content and characteristics of the \"corpus.txt\" file.  The analysis answers these implicit questions:\n",
        "\n",
        "* **What is the basic structure of the text?**  The text is comprised of a single, very long sentence (581 words) containing 583 words and 3889 characters.\n",
        "* **What are the most frequent words?** The most frequent words, excluding common stop words, relate to NCAA March Madness (e.g., tournament, byu, seed, march, regional, madness).\n",
        "* **What is the general topic of the text?** The text appears to be about NCAA March Madness predictions.\n",
        "\n",
        "\n",
        "### 2. Data Analysis Key Findings\n",
        "\n",
        "* **Single, Extremely Long Sentence:** The text consists of a single sentence with 581 words, which is highly unusual and suggests potential issues with sentence segmentation.\n",
        "* **Dominant Topic:** The most frequent words (excluding stop words) strongly indicate the text focuses on NCAA March Madness predictions, with mentions of specific teams (BYU, Duke, Florida, Arkansas) and tournament concepts (seed, regional).\n",
        "* **Average Sentence Length:**  The average sentence length is 581 words due to the single sentence.  Minimum and maximum sentence lengths are both 581 words.\n",
        "\n",
        "\n",
        "### 3. Insights or Next Steps\n",
        "\n",
        "* **Review Sentence Segmentation:** The single, extremely long sentence suggests an error in the sentence splitting regular expression.  Re-evaluate the regex or consider more robust sentence boundary detection methods.\n",
        "* **Further Topic Modeling:**  While the analysis suggests a focus on March Madness, more sophisticated topic modeling techniques could reveal nuances or additional themes within the text.\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "5w2CM0Zuutfx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!jupyter nbconvert --to html Manual_NLP.ipynb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3mV8q5EFzt09",
        "outputId": "42fb9602-7efa-42f7-9773-0550ae06c346"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NbConvertApp] Converting notebook Manual_NLP.ipynb to html\n",
            "[NbConvertApp] ERROR | Notebook JSON is invalid: Additional properties are not allowed ('errorDetails' was unexpected)\n",
            "\n",
            "Failed validating 'additionalProperties' in error:\n",
            "\n",
            "On instance['cells'][2]['outputs'][0]:\n",
            "{'ename': 'ModuleNotFoundError',\n",
            " 'errorDetails': {'actions': [{'action': 'open_url',\n",
            "                               'actionText': 'Open Examples',\n",
            "                               'url': '/notebooks/snippets/importing_libraries.ipynb'}]},\n",
            " 'evalue': \"No module named 'corpus'\",\n",
            " 'output_type': 'error',\n",
            " 'traceback': ['\\x1b[0;31m---------------------------------------------------------...',\n",
            "               '\\x1b[0;31mModuleNotFoundError\\x1b[0m                       '\n",
            "               'Traceback (...',\n",
            "               '\\x1b[0;32m<ipython-input-7-9f104b9f6dd0>\\x1b[0m in '\n",
            "               '\\x1b[0;36m<cell line: ...',\n",
            "               '\\x1b[0;31mModuleNotFoundError\\x1b[0m: No module named '\n",
            "               \"'corpus'\",\n",
            "               '',\n",
            "               '\\x1b[0;31m---------------------------------------------------------...']}\n",
            "[NbConvertApp] Writing 320344 bytes to Manual_NLP.html\n"
          ]
        }
      ]
    }
  ]
}